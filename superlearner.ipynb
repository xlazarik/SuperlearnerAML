{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e895586-1274-4b13-a031-4c799015d1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -f http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "668aebb2-473c-4835-a538-b773ff24348c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; OpenJDK 64-Bit Server VM JBR-11.0.13.7-1751.21-jcef (build 11.0.13+7-b1751.21, mixed mode)\n",
      "  Starting server from D:\\Archivos de programa\\Anaconda3\\envs\\Master_1\\Lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: C:\\Users\\david\\AppData\\Local\\Temp\\tmp73tmtmmz\n",
      "  JVM stdout: C:\\Users\\david\\AppData\\Local\\Temp\\tmp73tmtmmz\\h2o_david_started_from_python.out\n",
      "  JVM stderr: C:\\Users\\david\\AppData\\Local\\Temp\\tmp73tmtmmz\\h2o_david_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-1.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-1 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-1 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-1 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table th,\n",
       "#h2o-table-1 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>04 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Europe/Paris</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.46.0.6</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>2 months and 4 days</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_david_9k1xw8</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>3.979 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.10.13 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  -----------------------------\n",
       "H2O_cluster_uptime:         04 secs\n",
       "H2O_cluster_timezone:       Europe/Paris\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.46.0.6\n",
       "H2O_cluster_version_age:    2 months and 4 days\n",
       "H2O_cluster_name:           H2O_from_python_david_9k1xw8\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    3.979 Gb\n",
       "H2O_cluster_total_cores:    8\n",
       "H2O_cluster_allowed_cores:  8\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.10.13 final\n",
       "--------------------------  -----------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h2o\n",
    "from h2o.estimators import (\n",
    "    H2OGeneralizedLinearEstimator, \n",
    "    H2ORandomForestEstimator, \n",
    "    H2OGradientBoostingEstimator, \n",
    "    H2ONaiveBayesEstimator,\n",
    "    H2OStackedEnsembleEstimator,\n",
    "    H2ODeepLearningEstimator\n",
    "\n",
    ")\n",
    "from h2o.frame import H2OFrame\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f58eaa-76e0-4f1f-9a27-1f226a47557c",
   "metadata": {},
   "source": [
    "### GLOBAL PRESETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "e7bc9feb-07d7-4b1e-8378-4d9f765c0b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "N_FOLDS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a6973e-a3e7-4b1b-bcda-541feb7a8d63",
   "metadata": {},
   "source": [
    "### DATA LOADING AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d8d9852-ce43-4b48-b1aa-f6701bf176bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "63c2ad27-6da8-404c-9f2c-e4d7cf54b768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vladi\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>word_freq_conference</th>\n",
       "      <th>char_freq_%3B</th>\n",
       "      <th>char_freq_%28</th>\n",
       "      <th>char_freq_%5B</th>\n",
       "      <th>char_freq_%21</th>\n",
       "      <th>char_freq_%24</th>\n",
       "      <th>char_freq_%23</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  word_freq_conference  char_freq_%3B  \\\n",
       "0             0.00            0.00  ...                   0.0           0.00   \n",
       "1             0.00            0.94  ...                   0.0           0.00   \n",
       "2             0.64            0.25  ...                   0.0           0.01   \n",
       "3             0.31            0.63  ...                   0.0           0.00   \n",
       "4             0.31            0.63  ...                   0.0           0.00   \n",
       "\n",
       "   char_freq_%28  char_freq_%5B  char_freq_%21  char_freq_%24  char_freq_%23  \\\n",
       "0          0.000            0.0          0.778          0.000          0.000   \n",
       "1          0.132            0.0          0.372          0.180          0.048   \n",
       "2          0.143            0.0          0.276          0.184          0.010   \n",
       "3          0.137            0.0          0.137          0.000          0.000   \n",
       "4          0.135            0.0          0.135          0.000          0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  \n",
       "0                       278  \n",
       "1                      1028  \n",
       "2                      2259  \n",
       "3                       191  \n",
       "4                       191  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data = fetch_openml(data_id=44, as_frame=True)  \n",
    "spam_df = spam_data.frame\n",
    "# Split into features and target\n",
    "X = spam_df.iloc[:, :-1]  # All columns except the last are features\n",
    "y = spam_df.iloc[:, -1]   # The last column is the target (spam or not)\n",
    "\n",
    "# Convert target to numeric\n",
    "y = y.astype(int)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)\n",
    "\n",
    "# Convert to H2O Frames\n",
    "h2o_train = H2OFrame(pd.DataFrame(X_train).assign(label=y_train.values))\n",
    "h2o_test = H2OFrame(pd.DataFrame(X_test).assign(label=y_test.values))\n",
    "\n",
    "h2o_train['label'] = h2o_train['label'].asfactor()\n",
    "h2o_test['label'] = h2o_test['label'].asfactor()\n",
    "\n",
    "# Example of dataset\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c91792-1ec8-4fca-b714-c024a9df1177",
   "metadata": {},
   "source": [
    "### Is SPAM class underepresented?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "820d0484-447e-47d5-a571-b81da3a9ce15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records containing spam: 1813\n",
      "Records not containing spam: 2788\n"
     ]
    }
   ],
   "source": [
    "print(f\"Records containing spam: {len(spam_df[spam_df['class'] == '1'])}\")\n",
    "print(f\"Records not containing spam: {len(spam_df[spam_df['class'] == '0'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223e2513-612b-43f5-8e60-557e13eb01f8",
   "metadata": {},
   "source": [
    "### BASE LEARNERS - SPECIFICATION\n",
    "\n",
    "When choosing the base learners for the first layer we have considered the properties of the dataset and the task in this case being binary classification. As the paper XXX suggests \"the first layer should consist of diverse algorithms with different inductive biases to ensure a rich set of predictions for the meta-learner\".\n",
    "\n",
    "We have selected: \n",
    "\n",
    "**Random Forest:**\n",
    "\n",
    "Because they are robust to overfitting on datasets with many features and they handle noisy or irrelevant features well, which is can be a thing in this case.\n",
    "\n",
    "**Generalized Linear Model - Logistic regression:**\n",
    "\n",
    "We chose to add it because it's a simple yet effective baseline model, especially logistic regression for binary classification. It should provide a low-variance learner to complement the other high-variance ones.\n",
    "\n",
    "**Deep Learning (H20's MLP):**\n",
    "\n",
    "We add the neural networks, because of it's flexibility so it could capture non-linear relationships which should broaden the diversity of the stacks prediction.\n",
    "\n",
    "**Naive Bayes:**\n",
    "\n",
    "Why: Spam datasets often benefit from Naive Bayes since it assumes independence among features and thus might capture something more general than the other models.\n",
    "\n",
    "**Gradient Boosting Machines:**\n",
    "\n",
    "We choose them as another complement ensemble method that can capture rather complex relationship and so maybe overfit more to the data.\n",
    "\n",
    "\n",
    "We assume the simpler models like naive bayes and logistic regression should bring in the stack a more general view without focusing too much on the quirks in the data and to balance it out we have selected a more accurate and flexible methods like MLP or GBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "8f66af53-4ce2-4ae7-87f2-b13466b994d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_learners = {\n",
    "    \"LogisticRegression\": H2OGeneralizedLinearEstimator(family=\"binomial\", nfolds=N_FOLDS, seed=42, keep_cross_validation_predictions=True),\n",
    "    \"RandomForest\": H2ORandomForestEstimator(ntrees=50, max_depth=10, nfolds=N_FOLDS, seed=42, keep_cross_validation_predictions=True),\n",
    "    \"GradientBoosting\": H2OGradientBoostingEstimator(ntrees=50, max_depth=5, nfolds=N_FOLDS, seed=42, keep_cross_validation_predictions=True),\n",
    "    \"NaiveBayes\": H2ONaiveBayesEstimator(nfolds=N_FOLDS, seed=42, keep_cross_validation_predictions=True)\n",
    "}\n",
    "\n",
    "# TODO consider balance_class=True\n",
    "\n",
    "base_learners = {\n",
    "    \"LogisticRegression_binomial\": H2OGeneralizedLinearEstimator(\n",
    "        family=\"binomial\", nfolds=N_FOLDS, seed=42, keep_cross_validation_predictions=True\n",
    "    ),\n",
    "    \"RandomForest_50trees\": H2ORandomForestEstimator(\n",
    "        ntrees=50, max_depth=10, nfolds=N_FOLDS, seed=42, keep_cross_validation_predictions=True\n",
    "    ),\n",
    "        \"RandomForest_50trees_unbounded_D\": H2ORandomForestEstimator(\n",
    "        ntrees=50, max_depth=10, nfolds=N_FOLDS, seed=42, keep_cross_validation_predictions=True\n",
    "    ),\n",
    "    \"RandomForest_10trees\": H2ORandomForestEstimator(\n",
    "        ntrees=10, max_depth=10, nfolds=N_FOLDS, seed=42, keep_cross_validation_predictions=True\n",
    "    ),\n",
    "    \"RandomForest_10trees_unbounded_D\": H2ORandomForestEstimator(\n",
    "        ntrees=10, max_depth=10, nfolds=N_FOLDS, seed=42, keep_cross_validation_predictions=True\n",
    "    ),\n",
    "    # \"GradientBoosting\": H2OGradientBoostingEstimator(\n",
    "    #     ntrees=50, max_depth=5, nfolds=N_FOLDS, seed=42, keep_cross_validation_predictions=True, balance_classes=True\n",
    "    # ),\n",
    "    \"NaiveBayes\": H2ONaiveBayesEstimator(\n",
    "        nfolds=N_FOLDS, seed=42, keep_cross_validation_predictions=True\n",
    "    ),\n",
    "    \"NeuralNetwork\": H2ODeepLearningEstimator(\n",
    "        hidden=[32, 16], epochs=10, nfolds=N_FOLDS, seed=42, keep_cross_validation_predictions=True\n",
    "    ),\n",
    "    \"NeuralNetwork\": H2ODeepLearningEstimator(\n",
    "        hidden=[32, 16], epochs=10, nfolds=N_FOLDS, seed=42, keep_cross_validation_predictions=True\n",
    "    )\n",
    "}\n",
    "\n",
    "base_learners = {\n",
    "    \"LogisticRegression_binomial\": H2OGeneralizedLinearEstimator(\n",
    "        family=\"binomial\", nfolds=N_FOLDS, seed=42, keep_cross_validation_predictions=True\n",
    "    ),\n",
    "    \"RandomForest_50trees\": H2ORandomForestEstimator(\n",
    "        ntrees=50, max_depth=10, nfolds=N_FOLDS, seed=42, keep_cross_validation_predictions=True\n",
    "    ),\n",
    "        \"RandomForest_50trees_unbounded_D\": H2ORandomForestEstimator(\n",
    "        ntrees=50, nfolds=N_FOLDS, seed=42, keep_cross_validation_predictions=True\n",
    "    ),\n",
    "    \"RandomForest_10trees\": H2ORandomForestEstimator(\n",
    "        ntrees=10, max_depth=10, nfolds=N_FOLDS, seed=42, keep_cross_validation_predictions=True\n",
    "    ),\n",
    "    \"RandomForest_10trees_unbounded_D\": H2ORandomForestEstimator(\n",
    "        ntrees=10, nfolds=N_FOLDS, seed=42, keep_cross_validation_predictions=True\n",
    "    ),\n",
    "    # \"GradientBoosting\": H2OGradientBoostingEstimator(\n",
    "    #     ntrees=50, max_depth=5, nfolds=N_FOLDS, seed=42, keep_cross_validation_predictions=True, balance_classes=True\n",
    "    # ),\n",
    "    \"NaiveBayes\": H2ONaiveBayesEstimator(\n",
    "        nfolds=N_FOLDS, seed=42, keep_cross_validation_predictions=True\n",
    "    ),\n",
    "    \"NeuralNetwork\": H2ODeepLearningEstimator(\n",
    "        hidden=[32, 16], epochs=300, nfolds=N_FOLDS, seed=42, keep_cross_validation_predictions=True\n",
    "    ),\n",
    "    \"NeuralNetwork\": H2ODeepLearningEstimator(\n",
    "        hidden=[32, 32], epochs=300, nfolds=N_FOLDS, seed=42, keep_cross_validation_predictions=True\n",
    "    ),\n",
    "    \"NeuralNetwork\": H2ODeepLearningEstimator(\n",
    "        hidden=[32], epochs=300, nfolds=N_FOLDS, seed=42, keep_cross_validation_predictions=True\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c7f67b-4ad5-4d78-a96a-5b21ba3566c6",
   "metadata": {},
   "source": [
    "### BASE LEARNERS - TRAINING & EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "613da5ef-a22d-45ee-9443-57d2457c92e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression with 5-fold cross-validation...\n",
      "glm Model Build progress: |██████████████████████████████████████████████████████| (done) 100%\n",
      "Training RandomForest with 5-fold cross-validation...\n",
      "drf Model Build progress: |██████████████████████████████████████████████████████| (done) 100%\n",
      "Training GradientBoosting with 5-fold cross-validation...\n",
      "gbm Model Build progress: |██████████████████████████████████████████████████████| (done) 100%\n",
      "Training NaiveBayes with 5-fold cross-validation...\n",
      "naivebayes Model Build progress: |███████████████████████████████████████████████| (done) 100%\n",
      "LogisticRegression - F1-Score: 0.9125, AUC-PR: 0.9597, Accuracy (Test Set): 0.9251\n",
      "RandomForest - F1-Score: 0.9375, AUC-PR: 0.9790, Accuracy (Test Set): 0.9479\n",
      "GradientBoosting - F1-Score: 0.9490, AUC-PR: 0.9848, Accuracy (Test Set): 0.9566\n",
      "NaiveBayes - F1-Score: 0.8303, AUC-PR: 0.8182, Accuracy (Test Set): 0.8588\n",
      "Base Learner Results: {'LogisticRegression': {'F1-Score': 0.9125475285171102, 'AUC-PR': 0.9597465166788255, 'Accuracy': 0.9250814332247557}, 'RandomForest': {'F1-Score': 0.9375, 'AUC-PR': 0.9790110931537847, 'Accuracy': 0.9478827361563518}, 'GradientBoosting': {'F1-Score': 0.9489795918367347, 'AUC-PR': 0.9848112403394595, 'Accuracy': 0.9565689467969598}, 'NaiveBayes': {'F1-Score': 0.8302872062663186, 'AUC-PR': 0.8182424772642927, 'Accuracy': 0.8588490770901195}}\n"
     ]
    }
   ],
   "source": [
    "# Train each base learner using cross-validation\n",
    "for name, learner in base_learners.items():\n",
    "    print(f\"Training {name} with {N_FOLDS}-fold cross-validation...\")\n",
    "    learner.train(x=list(range(X_train.shape[1])), y=\"label\", training_frame=h2o_train)\n",
    "\n",
    "results = {}\n",
    "for name, learner in base_learners.items():\n",
    "    performance = learner.model_performance(test_data=h2o_test)\n",
    "    f1_score = performance.F1()[0][1]  \n",
    "    auc_pr = performance.aucpr()      \n",
    "    accuracy = performance.accuracy()[0][1]\n",
    "    results[name] = accuracy\n",
    "    results[name] = {\"F1-Score\": f1_score, \"AUC-PR\": auc_pr, \"Accuracy\": accuracy}\n",
    "    print(f\"{name} - F1-Score: {f1_score:.4f}, AUC-PR: {auc_pr:.4f}, Accuracy (Test Set): {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"Base Learner Results:\", results)\n",
    "\n",
    "base_models = list(base_learners.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "9ee8fbbf-aff0-4cd7-878d-84d6b0f0a07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_stack(base_learners, metalearner, h2o_train, h2o_test, X_train):\n",
    "\n",
    "    # TRAIN BASE LEARNERS\n",
    "    print(\"\\n>>> Training base learners:\\n\")\n",
    "    for name, learner in base_learners.items():\n",
    "        print(f\"    Training {name} with {N_FOLDS}-fold cross-validation...\")\n",
    "        learner.train(x=list(range(X_train.shape[1])), y=\"label\", training_frame=h2o_train)\n",
    "\n",
    "    # TRAIN THE METALEARNER\n",
    "    print(\"\\n>>> Training super learner:\\n\")\n",
    "    super_learner.train(x=list(range(X_train.shape[1])), y=\"label\", training_frame=h2o_train)\n",
    "\n",
    "    # EVAL BASE LEARNERS\n",
    "    print(\"\\n>>> Base learners' results:\\n\")\n",
    "    results = {}\n",
    "    for name, learner in base_learners.items():\n",
    "        performance = learner.model_performance(test_data=h2o_test)\n",
    "        f1_score = performance.F1()[0][1]  \n",
    "        auc_pr = performance.aucpr()      \n",
    "        accuracy = performance.accuracy()[0][1]\n",
    "        results[name] = accuracy\n",
    "        results[name] = {\"F1-Score\": f1_score, \"AUC-PR\": auc_pr, \"Accuracy\": accuracy}\n",
    "        print(f\"    {name} - F1-Score: {f1_score:.4f}, AUC-PR: {auc_pr:.4f}, Accuracy (Test Set): {accuracy:.4f}\")\n",
    "    \n",
    "    # EVAL THE METALEARNER\n",
    "    print(\"\\n>>> Metalearner's results:\\n\")\n",
    "    super_performance = super_learner.model_performance(test_data=h2o_test)\n",
    "    super_accuracy = super_performance.accuracy()[0][1]\n",
    "    super_f1 = super_performance.F1()[0][1]  \n",
    "    super_auc_pr = super_performance.aucpr()  \n",
    "    # print(f\"\\n    Super Learner - F1-Score: {super_f1:.4f}, AUC-PR: {super_auc_pr:.4f} | Super Learner Accuracy: {super_accuracy:.4f}\")\n",
    "    \n",
    "    \n",
    "    # print(\"\\nFinal Results Comparison:\")\n",
    "    # for name, metrics in results.items():\n",
    "    #     print(f\"{name} - F1-Score: {metrics['F1-Score']:.4f}, AUC-PR: {metrics['AUC-PR']:.4f}, Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "        \n",
    "    print(f\"    Super Learner - F1-Score: {super_f1:.4f}, AUC-PR: {super_auc_pr:.4f}, Accuracy: {super_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b97691-0800-43f5-944e-4579f0563eb7",
   "metadata": {},
   "source": [
    "### METALEARNER - TRAINING & EVALUATION\n",
    "\n",
    "\n",
    "For the metalearner we principally selected two possible options for testing:\n",
    "\n",
    "**GLM:**\n",
    "\n",
    "We chose logistic regression because it is simple and interpretable and as a meta-learner we want it just combine the predictions of individual learners by weighting the them reducing the risk of overfitting when combining predictions. So in this case we are more focused on finding the best combination of predictions rather than adding more complexity.\n",
    "\n",
    "**Gradient Boosting Machine / MLP:**\n",
    "\n",
    "As an alternative second option we wanted something stronger, a bit of a bigger hammer sort to say, especially for our stacks which are more diverse in which case their predictions could be more complex, so they could capture non-linear relationships among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03d103a3-1605-4928-8931-db29b9bac0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Super Learner...\n",
      "stackedensemble Model Build progress: |██████████████████████████████████████████| (done) 100%\n",
      "\n",
      "Super Learner - F1-Score: 0.9537, AUC-PR: 0.9839 | Super Learner Accuracy: 0.9609\n",
      "\n",
      "Final Results Comparison:\n",
      "LogisticRegression - F1-Score: 0.9125, AUC-PR: 0.9597, Accuracy: 0.9251\n",
      "RandomForest - F1-Score: 0.9375, AUC-PR: 0.9790, Accuracy: 0.9479\n",
      "GradientBoosting - F1-Score: 0.9490, AUC-PR: 0.9848, Accuracy: 0.9566\n",
      "NaiveBayes - F1-Score: 0.8303, AUC-PR: 0.8182, Accuracy: 0.8588\n",
      "Super Learner - F1-Score: 0.9537, AUC-PR: 0.9839, Accuracy: 0.9609\n"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "super_learner = H2OStackedEnsembleEstimator(\n",
    "    base_models=base_models,\n",
    "    metalearner_algorithm=\"glm\"  # Uses Logistic Regression as the metalearner\n",
    ")\n",
    "\n",
    "# super_learner = H2OStackedEnsembleEstimator(\n",
    "#     base_models=base_models,\n",
    "#     metalearner_algorithm=\"deeplearning\"  # Uses Logistic Regression as the metalearner\n",
    "# )\n",
    "\n",
    "print(\"\\nTraining Super Learner:\\n\")\n",
    "super_learner.train(x=list(range(X_train.shape[1])), y=\"label\", training_frame=h2o_train)\n",
    "\n",
    "# EVAL\n",
    "super_performance = super_learner.model_performance(test_data=h2o_test)\n",
    "super_accuracy = super_performance.accuracy()[0][1]\n",
    "super_f1 = super_performance.F1()[0][1]  \n",
    "super_auc_pr = super_performance.aucpr()  \n",
    "# print(f\"\\nSuper Learner - F1-Score: {super_f1:.4f}, AUC-PR: {super_auc_pr:.4f} | Super Learner Accuracy: {super_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nFinal Results Comparison:\\n\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"    {name} - F1-Score: {metrics['F1-Score']:.4f}, AUC-PR: {metrics['AUC-PR']:.4f}, Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "    \n",
    "print(f\"    Super Learner - F1-Score: {super_f1:.4f}, AUC-PR: {super_auc_pr:.4f}, Accuracy: {super_accuracy:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "f7e61dcd-28d1-4a92-8fc1-cfba19777b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Training base learners:\n",
      "\n",
      "    Training LogisticRegression_binomial with 5-fold cross-validation...\n",
      "glm Model Build progress: |██████████████████████████████████████████████████████| (done) 100%\n",
      "    Training RandomForest_50trees with 5-fold cross-validation...\n",
      "drf Model Build progress: |██████████████████████████████████████████████████████| (done) 100%\n",
      "    Training RandomForest_50trees_unbounded_D with 5-fold cross-validation...\n",
      "drf Model Build progress: |██████████████████████████████████████████████████████| (done) 100%\n",
      "    Training RandomForest_10trees with 5-fold cross-validation...\n",
      "drf Model Build progress: |██████████████████████████████████████████████████████| (done) 100%\n",
      "    Training RandomForest_10trees_unbounded_D with 5-fold cross-validation...\n",
      "drf Model Build progress: |██████████████████████████████████████████████████████| (done) 100%\n",
      "    Training NaiveBayes with 5-fold cross-validation...\n",
      "naivebayes Model Build progress: |███████████████████████████████████████████████| (done) 100%\n",
      "    Training NeuralNetwork with 5-fold cross-validation...\n",
      "deeplearning Model Build progress: |█████████████████████████████████████████████| (done) 100%\n",
      "\n",
      ">>> Training super learner:\n",
      "\n",
      "stackedensemble Model Build progress: |██████████████████████████████████████████| (done) 100%\n",
      "\n",
      ">>> Base learners' results:\n",
      "\n",
      "    LogisticRegression_binomial - F1-Score: 0.9125, AUC-PR: 0.9597, Accuracy (Test Set): 0.9251\n",
      "    RandomForest_50trees - F1-Score: 0.9375, AUC-PR: 0.9790, Accuracy (Test Set): 0.9479\n",
      "    RandomForest_50trees_unbounded_D - F1-Score: 0.9501, AUC-PR: 0.9839, Accuracy (Test Set): 0.9577\n",
      "    RandomForest_10trees - F1-Score: 0.9235, AUC-PR: 0.9688, Accuracy (Test Set): 0.9370\n",
      "    RandomForest_10trees_unbounded_D - F1-Score: 0.9300, AUC-PR: 0.9756, Accuracy (Test Set): 0.9425\n",
      "    NaiveBayes - F1-Score: 0.8303, AUC-PR: 0.8182, Accuracy (Test Set): 0.8588\n",
      "    NeuralNetwork - F1-Score: 0.9366, AUC-PR: 0.9632, Accuracy (Test Set): 0.9468\n",
      "\n",
      ">>> Metalearner's results:\n",
      "\n",
      "    Super Learner - F1-Score: 0.9096, AUC-PR: 0.9690, Accuracy: 0.9240\n"
     ]
    }
   ],
   "source": [
    "train_evaluate_stack(base_learners, super_learner, h2o_train, h2o_test, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb35ae20-50da-4eb9-b567-8f5824082e39",
   "metadata": {},
   "source": [
    "## TRYING OUT GRID SEARCH FROM\n",
    "https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b66844c6-9dc5-493a-89fb-c1fb41759eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm Grid Build progress: |███████████████████████████████████████████████████████| (done) 100%\n",
      "stackedensemble Model Build progress: |██████████████████████████████████████████| (done) 100%\n",
      "\n",
      "Super Learner - F1-Score: 0.9096, AUC-PR: 0.9690 | Super Learner Accuracy: 0.9240\n",
      "\n",
      "Final Results Comparison:\n",
      "LogisticRegression - F1-Score: 0.9125, AUC-PR: 0.9597, Accuracy: 0.9251\n",
      "RandomForest - F1-Score: 0.9375, AUC-PR: 0.9790, Accuracy: 0.9479\n",
      "GradientBoosting - F1-Score: 0.9490, AUC-PR: 0.9848, Accuracy: 0.9566\n",
      "NaiveBayes - F1-Score: 0.8303, AUC-PR: 0.8182, Accuracy: 0.8588\n",
      "Super Learner - F1-Score: 0.9096, AUC-PR: 0.9690, Accuracy: 0.9240\n"
     ]
    }
   ],
   "source": [
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "\n",
    "# Specify GBM hyperparameters for the grid\n",
    "hyper_params = {\"learn_rate\": [0.01, 0.03],\n",
    "                \"max_depth\": [3, 4, 5, 6, 9],\n",
    "                \"sample_rate\": [0.7, 0.8, 0.9, 1.0],\n",
    "                \"col_sample_rate\": [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]}\n",
    "search_criteria = {\"strategy\": \"RandomDiscrete\", \"max_models\": 3, \"seed\": 1}\n",
    "\n",
    "# Train the grid\n",
    "grid = H2OGridSearch(model=H2OGradientBoostingEstimator(ntrees=10,\n",
    "                                                        seed=1,\n",
    "                                                        nfolds=N_FOLDS,\n",
    "                                                        fold_assignment=\"Modulo\",\n",
    "                                                        keep_cross_validation_predictions=True),\n",
    "                     hyper_params=hyper_params,\n",
    "                     search_criteria=search_criteria,\n",
    "                     grid_id=\"gbm_grid_binomial\")\n",
    "grid.train(x=list(range(X_train.shape[1])), y=\"label\", training_frame=h2o_train)\n",
    "\n",
    "# Train a stacked ensemble using the GBM grid\n",
    "super_learner = H2OStackedEnsembleEstimator(model_id=\"my_ensemble_gbm_grid_binomial\",\n",
    "                                       base_models=grid.model_ids)\n",
    "super_learner.train(x=list(range(X_train.shape[1])), y=\"label\", training_frame=h2o_train)\n",
    "\n",
    "\n",
    "\n",
    "super_performance = super_learner.model_performance(test_data=h2o_test)\n",
    "super_accuracy = super_performance.accuracy()[0][1]\n",
    "super_f1 = super_performance.F1()[0][1]  \n",
    "super_auc_pr = super_performance.aucpr()  \n",
    "print(f\"\\nSuper Learner - F1-Score: {super_f1:.4f}, AUC-PR: {super_auc_pr:.4f} | Super Learner Accuracy: {super_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nFinal Results Comparison:\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name} - F1-Score: {metrics['F1-Score']:.4f}, AUC-PR: {metrics['AUC-PR']:.4f}, Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "    \n",
    "print(f\"Super Learner - F1-Score: {super_f1:.4f}, AUC-PR: {super_auc_pr:.4f}, Accuracy: {super_accuracy:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
