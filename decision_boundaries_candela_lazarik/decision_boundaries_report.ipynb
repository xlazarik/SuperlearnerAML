{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2598b2ec-c778-4a94-960e-71728e4c6c0c",
   "metadata": {},
   "source": [
    "# OVERVIEW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e24c35-3146-41f1-be52-d59d265dab85",
   "metadata": {},
   "source": [
    "![title](images/overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f40b0b-7468-4dbc-8544-51d2cc4e82a1",
   "metadata": {},
   "source": [
    "# DETAILED COMPARISONS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81761f5a-0ae1-413b-8a11-a9ce76d82096",
   "metadata": {},
   "source": [
    "# NEAREST NEIGHBOURS\n",
    "\n",
    "In kNN the decision boundary is the dividing line or surface that separates the feature space into regions classified as one class versus another based on the proximity of a query point to the training points. The boundary can be highly non-linear and jagged, especially for low values of k, because each region is determined by the immediate nearest training points. Every data point essentially influences the shape of the boundary.\n",
    "\n",
    "For 1-NN **(k=1)**, the decision boundary is very sensitive to noise and may overfit the training data. The decision boundary for 1-NN forms a Voronoi diagram, where the feature space is divided into regions based on the closest training point. The edges of these regions represent the decision boundaries.\n",
    "\n",
    "As **k increases**, the boundary becomes smoother because the classifier takes a majority vote, and it becomes less sensitive to individual noisy points.\n",
    "\n",
    "The shape of regions can be also influenced by the choice of **distance metric**.\n",
    "\n",
    "It can handle also not linearly separable data points and more complex shapes as it follows the borders of the regions according to the class distributions in space.\n",
    "\n",
    "![title](images/nearest_neighbors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb8130d-6ebf-47e5-bf6a-147b181dcc23",
   "metadata": {},
   "source": [
    "# DECISION TREE\n",
    "\n",
    "The decision boundary for a decision tree classifier represents the regions in the feature space where the model assigns the same class label.\n",
    "\n",
    "A decision tree splits the feature space into regions using axis-aligned splits (e.g., \"feature >5\" or \"feature  ≤3\" or based on a category 0/1).\n",
    "The resulting decision boundary is a collection of rectangles (in 2D) or hyper-rectangles (in higher dimensions) - intersections of hyperplanes.\n",
    "\n",
    "In 2D where the decision boundary is a combination of axis-aligned segments or rectangles, each rectangle corresponds to a leaf node of the tree, and all points within it are assigned the same label.\n",
    "\n",
    "**Shallow Tree:** Fewer splits -> Large rectangular regions.\n",
    "\n",
    "**Deep Tree:** More splits -> Smaller, finely divided regions -> Captures finer details of the data distribution but can overfit.\n",
    "\n",
    "![title](images/decision_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cde7f8a-7eab-4836-80b6-00f17be1af35",
   "metadata": {},
   "source": [
    "# RANDOM FOREST\n",
    "\n",
    "Each tree generates its own decision boundary, which as we said is typically axis-aligned and piecewise-rectangular. The boundary is more detailed in regions where the training data is dense and less so in sparse regions. This reflects how trees adapt to local patterns.\n",
    "\n",
    "**More trees** Smoother and more stable boundary, as more trees decrease the influence of outliers and noise.\n",
    "\n",
    "**Fewer trees:** Slightly noisier, jagged boundaries resembling individual tree splits.\n",
    "\n",
    "**Shallow trees:** Coarser boundaries, unable to capture small patterns.\n",
    "\n",
    "**Deep trees:** Finer boundaries, and overfitting risk for individual trees is mitigated by averaging.\n",
    "\n",
    "![title](images/random_forest.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6126990-1da8-4e0c-b747-5815e9b90d1c",
   "metadata": {},
   "source": [
    "# NAIVE BAYES\n",
    "\n",
    "The decision boundary separates regions where one class has a higher probability than the others.\n",
    "Mathematically, it’s where P(y1∣X)=P(y2∣X), meaning the probabilities of two classes are equal.\n",
    "\n",
    "Boundaries, when assuming a gaussian distribtion, are often linear or moderately curved, which reflects the simplicity of the model.\n",
    "<!-- The type of feature distributions we assume (Gaussian, multinomial, etc.) can directly influence the boundary. -->\n",
    "If one class has a much higher prior probability P(y), the decision boundary skews in favor of that class.\n",
    "\n",
    "\n",
    "![title](images/naive_bayes01.png) \n",
    "![title](images/naive_bayes02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4131cbbc-6e02-4fd3-8963-8b5ed762ffa4",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION\n",
    "The region where the predicted probability of one class equals that of another (typically at 50%).\n",
    "Logistic regression produces a linear decision boundary because it is based on a linear combination of input features passed through a sigmoid function.\n",
    "Using logistic regression to classify two classes in a 2D plane produces the decision boundary as a straight line. In higher dimensions, it becomes a hyperplane. Due to the linear nature of DB it cannot effectively separate non-linear patterns such as moons or blobs.\n",
    "\n",
    "<!-- Regularization can shrink the coefficients leading to less steepness until we reach the horizontal line -> which gives us the average for the dependent variable. -->\n",
    "With too much regularization, the model may oversimplify, leading to underfitting and a less effective boundary.\n",
    "\n",
    "![title](images/logistic_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7b0b08-347d-4d20-9d85-79caa5469c27",
   "metadata": {},
   "source": [
    "# Linear discriminant analysis\n",
    "\n",
    "It tries to find the linear combination of features that best separates the classes. This results in a linear decision boundary.\n",
    "\n",
    "The decision boundary is a straight line (in 2D) or a hyperplane (in higher dims) that separates the two classes.\n",
    "The decision boundary is perpendicular to the line connecting the class means and located based on the prior probabilities and covariances. The method tries to separate the two means as much as possible.\n",
    "\n",
    "Since it produces a linear decision boundary it has issues separating non-linear patterns such as overlapping distributions.\n",
    "\n",
    "![title](images/lda01.png)\n",
    "\n",
    "![title](images/lda02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442c31d2-efe3-49cc-96e3-eaeaf754d2fa",
   "metadata": {},
   "source": [
    "# Quadratic discriminant analysis\n",
    "\n",
    "QDA allows each class to have its own covariance matrix instead of assuming a shared covariance matrix for all classes.\n",
    "This results in quadratic decision boundaries rather than linear ones. \n",
    "QDA is more flexible than LDA and can model some non-linear relationships between features and classes, so it can also handle a more complex datasets.\n",
    "\n",
    "![title](images/lda01.png)\n",
    "\n",
    "![title](images/lda02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca3922a-49e5-46d3-9131-a7a03e78fb28",
   "metadata": {},
   "source": [
    "# MLP\n",
    "\n",
    "The hidden layer's non-linearity allows the MLP to model non-linear decision boundaries. \n",
    "\n",
    "\n",
    "\n",
    "For a single neuron it can specify decision boundary is linear because the single neuron applies a weighted sum of the inputs so it separates the space in two subspaces. By introducing a hidden layer we are introducing an arbitrary number of these space divisions which we can combine in the output neuron.\n",
    "So hidden neurons collectively partition the feature space into regions and combine them to form the curved boundary.\n",
    "\n",
    "For linearly separable data (e.g., points separable by a straight line), the decision boundary of an MLP can resemble that of logistic regression: a straight line or hyperplane. If the hidden layer has enough neurons, it can still introduce some slight curvature, but the problem doesn’t require it.\n",
    "\n",
    "With enough neurons in the hidden layer, an MLP can approximate any decision boundary (this is a result of the Universal Approximation Theorem). However, the number of neurons and training data must be sufficient to represent the complexity of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e4ec7b-bc7f-4fb3-8b56-e9761b0746cd",
   "metadata": {},
   "source": [
    "### How hyperparameters affect the decision boundary\n",
    "\n",
    "#### Number of neurons in the hidden layer:\n",
    "\n",
    "**Few neurons:** The model may underfit and produce overly simple decision boundaries (e.g., straight or slightly curved lines).\n",
    "\n",
    "**More neurons:** Allows the model to capture more complex decision boundaries.\n",
    "\n",
    "For example, with moons data, adding neurons enables the network to curve and adapt to the crescent shapes.\n",
    "\n",
    "**Too many neurons:** Increases the risk of overfitting, where the decision boundary becomes unnecessarily complex and starts fitting noise.\n",
    "\n",
    "![title](images/mlp_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ed83f-82e9-4429-95b0-4ea39256ddff",
   "metadata": {},
   "source": [
    "#### Regularization\n",
    "Alpha is a parameter for regularization term, aka penalty term, that combats overfitting by constraining the size of the weights. Increasing alpha may fix high variance (a sign of overfitting) by encouraging smaller weights, resulting in a decision boundary plot that appears with lesser curvatures. Similarly, decreasing alpha may fix high bias (a sign of underfitting) by encouraging larger weights, potentially resulting in a more complicated decision boundary.\n",
    "\n",
    "![title](images/mlp_alpha.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7510f45d-12a2-4818-9186-36cebda67d44",
   "metadata": {},
   "source": [
    "#### Learning Rate:\n",
    "\n",
    "A low learning rate might cause the decision boundary to converge too slowly, resulting in suboptimal performance.\n",
    "A high learning rate might lead to instability, causing the boundary to oscillate without finding a good fit.\n",
    "\n",
    "![title](images/mlp_lr.png)\n",
    "\n",
    "#### Number of Training Epochs:\n",
    "\n",
    "Training for too few epochs can lead to underfitting, with an overly simple boundary.\n",
    "Too many epochs can lead to overfitting, where the boundary becomes unnecessarily detailed and complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258c3a9e-773e-443f-b13e-23c6bd29fefd",
   "metadata": {},
   "source": [
    "### What DB does MLP with more than 1 hidden layer generate?\n",
    "\n",
    "![title](images/mlp_multilayer.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e27e38-786e-4b78-9d58-0b90793ff406",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "For linearly separable data, the SVM constructs a straight line (in 2D) or a hyperplane (in higher dimensions) as the decision boundary.\n",
    "\n",
    "When data is not linearly separable, we can use different (than linear) kernels to map the data into a higher-dimensional space where it becomes linearly separable. The decision boundary is then a non-linear curve in the original input space but linear in the feature space.\n",
    "\n",
    "In the case of a **polynomial kernel**, with the increasing degree of the polynomial, the SVM can fit more detailed/complex (e.g., twisting more) boundaries between classes, but we also risk overfitting to the data.\n",
    "\n",
    "**Margin Softness (Slack Variable):**\n",
    "For datasets with overlapping classes, SVM can use slack variables to allow some points to violate the margin.\n",
    "A softer margin leads to smoother boundaries that tolerate noise, while a harder margin enforces stricter separation.\n",
    "\n",
    "![title](images/svm_overview.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7065bdf-c064-4618-84b7-c2fa5552cf69",
   "metadata": {},
   "source": [
    "#### Regularization Parameter (C):\n",
    "C controls the trade-off between maximizing the margin and minimizing misclassification:\n",
    "\n",
    "**Small C:** Increases margin size by allowing some misclassifications. This leads to a simpler, smoother decision boundary that generalizes better.\n",
    "\n",
    "**Large C:** Reduces the margin to classify all training points correctly. This can result in a complex boundary that overfits the training data.\n",
    "\n",
    "![title](images/svm_reg_c.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f763ee-343d-414d-9acd-c9fdfd7d753a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Gamma (for RBF Kernel):\n",
    "Gamma controls the influence of individual data points on the decision boundary:\n",
    "\n",
    "**Low Gamma:** Each data point has a far-reaching influence, leading to smoother, more generalized decision boundaries.\n",
    "\n",
    "**High Gamma:** Each data point has a limited influence, resulting in a highly flexible decision boundary that can overfit the data.\n",
    "\n",
    "\n",
    "![title](images/svm_gamma.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cea6cd0-3141-47c8-81e3-a2ea70cecd72",
   "metadata": {},
   "source": [
    "#### How does regularization parameter C affect the margins?\n",
    "\n",
    "A larger value of C results in a narrower margin and stricter classification, while a smaller value of C allows for a larger margin and more misclassification.\n",
    "\n",
    "\n",
    "![title](images/svm_margins_C.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
